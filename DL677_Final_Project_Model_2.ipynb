{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-qH_NSqSab0",
        "outputId": "bf738d8e-a418-41d5-c1ef-ccc5bce57553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import random\n",
        "\n",
        "# Define a simple dataset class\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_to_idx):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = [self.word_to_idx[word] for word in self.texts[idx].split()]\n",
        "\n",
        "        # Try to convert the label to an integer, handle non-numeric labels\n",
        "        try:\n",
        "            label = torch.as_tensor(int(self.labels[idx])).clone().detach()\n",
        "        except ValueError:\n",
        "            # Handle the case where the label is not a valid integer\n",
        "            label = torch.as_tensor(0)  # Set a default value or handle it as appropriate\n",
        "\n",
        "        return {'text': torch.LongTensor(text), 'label': label}\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts = [item['text'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    # Pad sequences to the same length within each batch\n",
        "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'text': padded_texts, 'label': torch.stack(labels)}\n",
        "\n",
        "def read_data_from_csv(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)  # Skip header if exists\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) >= 2:\n",
        "                texts.append(parts[0])\n",
        "                labels.append(parts[1].strip('\\\"'))\n",
        "    return texts, labels\n",
        "\n",
        "# Text preprocessing\n",
        "class EmotionAnalysisModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(EmotionAnalysisModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.batch_norm = nn.BatchNorm1d(hidden_dim)  # Add batch normalization\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        lstm_out = self.batch_norm(lstm_out)  # Apply batch normalization\n",
        "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "\n",
        "# Read data from CSV file\n",
        "file_path = '/content/gdrive/MyDrive/DS677 - Fall 23 - DL Project - Paresh, Ojaswi, Dinesh/TextAndEmotions.csv'\n",
        "texts, labels = read_data_from_csv(file_path)\n",
        "\n",
        "# Create vocabulary and word_to_idx mapping\n",
        "vocab = set(' '.join(texts).split())\n",
        "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}  # Add 1 to reserve index 0 for padding\n",
        "\n",
        "# Update the dataset and pad sequences\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = EmotionDataset(train_texts, train_labels, word_to_idx)\n",
        "test_dataset = EmotionDataset(test_texts, test_labels, word_to_idx)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmotionAnalysisModel(vocab_size=len(word_to_idx) + 1, embedding_dim=50, hidden_dim=100, output_dim=len(set(labels))).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        text, label = batch['text'].to(device), batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "    # Update the learning rate based on training loss\n",
        "    scheduler.step(total_loss / len(train_loader))\n",
        "\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        text, label = batch['text'].to(device), batch['label'].to(device)\n",
        "        output = model(text)\n",
        "        predictions = torch.argmax(output, dim=1)\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(label.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQxP8mbRSaes",
        "outputId": "cbf04ec4-cb44-47dd-f50d-e829075c1eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 5.212891896565755\n",
            "Epoch 2/50, Loss: 4.99224673377143\n",
            "Epoch 3/50, Loss: 4.902364730834961\n",
            "Epoch 4/50, Loss: 4.60579087999132\n",
            "Epoch 5/50, Loss: 4.311837037404378\n",
            "Epoch 6/50, Loss: 3.9687681198120117\n",
            "Epoch 7/50, Loss: 3.5991027620103626\n",
            "Epoch 8/50, Loss: 3.12363330523173\n",
            "Epoch 9/50, Loss: 2.501327223247952\n",
            "Epoch 10/50, Loss: 2.0091420544518366\n",
            "Epoch 11/50, Loss: 1.4027652210659451\n",
            "Epoch 12/50, Loss: 1.1353367567062378\n",
            "Epoch 13/50, Loss: 0.675278902053833\n",
            "Epoch 14/50, Loss: 0.5552616500192218\n",
            "Epoch 15/50, Loss: 0.3590041597684224\n",
            "Epoch 16/50, Loss: 0.19627143401238653\n",
            "Epoch 17/50, Loss: 0.13862000323004192\n",
            "Epoch 18/50, Loss: 0.12142143522699674\n",
            "Epoch 19/50, Loss: 0.08268820266756746\n",
            "Epoch 20/50, Loss: 0.059918465092778206\n",
            "Epoch 21/50, Loss: 0.04743958595726225\n",
            "Epoch 22/50, Loss: 0.09386506314492887\n",
            "Epoch 23/50, Loss: 0.04212546803885036\n",
            "Epoch 24/50, Loss: 0.034734474081132144\n",
            "Epoch 25/50, Loss: 0.04235730232256982\n",
            "Epoch 26/50, Loss: 0.023594688727623887\n",
            "Epoch 27/50, Loss: 0.02539259402288331\n",
            "Epoch 28/50, Loss: 0.01916065610324343\n",
            "Epoch 29/50, Loss: 0.03082093187711305\n",
            "Epoch 30/50, Loss: 0.025313820224255323\n",
            "Epoch 31/50, Loss: 0.018674598230669897\n",
            "Epoch 32/50, Loss: 0.020553952341692314\n",
            "Epoch 33/50, Loss: 0.02424334010316266\n",
            "Epoch 34/50, Loss: 0.017769804845253628\n",
            "Epoch 35/50, Loss: 0.015171972698428564\n",
            "Epoch 36/50, Loss: 0.03093576767585344\n",
            "Epoch 37/50, Loss: 0.08671659797740479\n",
            "Epoch 38/50, Loss: 0.051407156706166766\n",
            "Epoch 39/50, Loss: 0.014900382944486208\n",
            "Epoch 40/50, Loss: 0.03975348195268048\n",
            "Epoch 41/50, Loss: 0.024840969204281766\n",
            "Epoch 42/50, Loss: 0.0878054055178331\n",
            "Epoch 43/50, Loss: 0.10371428821235895\n",
            "Epoch 44/50, Loss: 0.02248115252910389\n",
            "Epoch 45/50, Loss: 0.013741839909926057\n",
            "Epoch 46/50, Loss: 0.010360020186959041\n",
            "Epoch 47/50, Loss: 0.013167030695411894\n",
            "Epoch 48/50, Loss: 0.009693657785343627\n",
            "Epoch 49/50, Loss: 0.006986469418431322\n",
            "Epoch 50/50, Loss: 0.008784590871073306\n",
            "Test Accuracy: 95.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample inputs\n",
        "sample_inputs = [\n",
        "    \"I love the new feature!\",\n",
        "    \"I am satisfied with the customer service.\",\n",
        "    \"This movie is not good!\",\n",
        "    \"This software have lots of features.\",\n",
        "    \"Incredible customer experience!\",\n",
        "    \"I am anxious\"\n",
        "]\n",
        "\n",
        "# Predict emotions for each sample input\n",
        "for sample_input in sample_inputs:\n",
        "    # Preprocess the sample input\n",
        "    sample_input_indices = [word_to_idx[word] for word in sample_input.split() if word in word_to_idx]\n",
        "\n",
        "    # Check if the sample input is empty after filtering\n",
        "    if not sample_input_indices:\n",
        "        print(f\"No valid words found in the sample input: {sample_input}\")\n",
        "    else:\n",
        "        # Convert to tensor and add batch dimension\n",
        "        sample_input_tensor = torch.LongTensor(sample_input_indices).unsqueeze(0).to(device)\n",
        "\n",
        "        # Pass through the trained model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(sample_input_tensor)\n",
        "\n",
        "        # Interpret the model's output\n",
        "        predicted_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # Map the predicted class to the corresponding emotion label\n",
        "        emotion_labels = {0: \"Negative\", 1: \"Positive\"}  # Update with your specific labels\n",
        "        predicted_emotion = emotion_labels.get(predicted_class, \"Unknown\")\n",
        "\n",
        "        print(f\"The predicted emotion for the sample input '{sample_input}' is: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T-UkFaFSsJU",
        "outputId": "5a603190-c650-4599-8942-a47496cceb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted emotion for the sample input 'I love the new feature!' is: Positive\n",
            "The predicted emotion for the sample input 'I am satisfied with the customer service.' is: Positive\n",
            "The predicted emotion for the sample input 'This movie is not good!' is: Negative\n",
            "The predicted emotion for the sample input 'This software have lots of features.' is: Negative\n",
            "The predicted emotion for the sample input 'Incredible customer experience!' is: Negative\n",
            "The predicted emotion for the sample input 'I am anxious' is: Positive\n"
          ]
        }
      ]
    }
  ]
}