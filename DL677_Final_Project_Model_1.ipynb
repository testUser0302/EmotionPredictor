{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY0tXe07vh4k",
        "outputId": "2291ea4e-d9d1-41dc-882d-14e1200ddd44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEY3YfXdsRoc",
        "outputId": "a13f9fea-8943-417b-bc71-abbaf00efdb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 4.547698232862684\n",
            "Epoch 2/50, Loss: 2.958804806073507\n",
            "Epoch 3/50, Loss: 1.4416541390948825\n",
            "Epoch 4/50, Loss: 0.6148142417271932\n",
            "Epoch 5/50, Loss: 0.4744644496175978\n",
            "Epoch 6/50, Loss: 0.4506297939353519\n",
            "Epoch 7/50, Loss: 0.45748965938886005\n",
            "Epoch 8/50, Loss: 0.40766768985324436\n",
            "Epoch 9/50, Loss: 0.4111311700608995\n",
            "Epoch 10/50, Loss: 0.406077245871226\n",
            "Epoch 11/50, Loss: 0.40343988604015774\n",
            "Epoch 12/50, Loss: 0.3970780074596405\n",
            "Epoch 13/50, Loss: 0.4070357580979665\n",
            "Epoch 14/50, Loss: 0.4112670554055108\n",
            "Epoch 15/50, Loss: 0.40231814980506897\n",
            "Epoch 16/50, Loss: 0.40141647722986007\n",
            "Epoch 17/50, Loss: 0.4043105145295461\n",
            "Epoch 18/50, Loss: 0.410246878862381\n",
            "Epoch 19/50, Loss: 0.3874448438485463\n",
            "Epoch 20/50, Loss: 0.40347252951727974\n",
            "Epoch 21/50, Loss: 0.3785329858462016\n",
            "Epoch 22/50, Loss: 0.38605739838547176\n",
            "Epoch 23/50, Loss: 0.39331305358144975\n",
            "Epoch 24/50, Loss: 0.37998779283629525\n",
            "Epoch 25/50, Loss: 0.3797275788254208\n",
            "Epoch 26/50, Loss: 0.3898761636681027\n",
            "Epoch 27/50, Loss: 0.3883558064699173\n",
            "Epoch 28/50, Loss: 0.3734783000416226\n",
            "Epoch 29/50, Loss: 0.3768281936645508\n",
            "Epoch 30/50, Loss: 0.37319568130705094\n",
            "Epoch 31/50, Loss: 0.3560303780767653\n",
            "Epoch 32/50, Loss: 0.3446658220556047\n",
            "Epoch 33/50, Loss: 0.3398974703417884\n",
            "Epoch 34/50, Loss: 0.340749376349979\n",
            "Epoch 35/50, Loss: 0.32340293294853634\n",
            "Epoch 36/50, Loss: 0.29859186377790237\n",
            "Epoch 37/50, Loss: 0.2958988199631373\n",
            "Epoch 38/50, Loss: 0.27048202024565804\n",
            "Epoch 39/50, Loss: 0.258365273475647\n",
            "Epoch 40/50, Loss: 0.21922836038801405\n",
            "Epoch 41/50, Loss: 0.20390544169478947\n",
            "Epoch 42/50, Loss: 0.1647645193669531\n",
            "Epoch 43/50, Loss: 0.14581284837590325\n",
            "Epoch 44/50, Loss: 0.13826308109694058\n",
            "Epoch 45/50, Loss: 0.11470513252748384\n",
            "Epoch 46/50, Loss: 0.09886708648668395\n",
            "Epoch 47/50, Loss: 0.08508327189419004\n",
            "Epoch 48/50, Loss: 0.08327979987694158\n",
            "Epoch 49/50, Loss: 0.07419805746111605\n",
            "Epoch 50/50, Loss: 0.065288371923897\n",
            "Test Accuracy: 91.67%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "# Define a simple dataset class\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, texts, labels, word_to_idx):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.word_to_idx = word_to_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = [self.word_to_idx[word] for word in self.texts[idx].split()]\n",
        "\n",
        "        # Try to convert the label to an integer, handle non-numeric labels\n",
        "        try:\n",
        "            label = torch.as_tensor(int(self.labels[idx])).clone().detach()\n",
        "        except ValueError:\n",
        "            # Handle the case where the label is not a valid integer\n",
        "            label = torch.as_tensor(0)  # Set a default value or handle it as appropriate\n",
        "\n",
        "        return {'text': torch.LongTensor(text), 'label': label}\n",
        "\n",
        "def collate_batch(batch):\n",
        "    texts = [item['text'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    # Pad sequences to the same length within each batch\n",
        "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {'text': padded_texts, 'label': torch.stack(labels)}\n",
        "\n",
        "def read_data_from_csv(file_path):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        next(file)  # Skip header if exists\n",
        "        for line in file:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) >= 2:\n",
        "                texts.append(parts[0])\n",
        "                labels.append(parts[1].strip('\\\"'))\n",
        "    return texts, labels\n",
        "\n",
        "# Text preprocessing\n",
        "class EmotionAnalysisModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(EmotionAnalysisModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "        out = self.fc(lstm_out)\n",
        "        return out\n",
        "\n",
        "# Read data from CSV file\n",
        "file_path = '/content/gdrive/MyDrive/DS677 - Fall 23 - DL Project - Paresh, Ojaswi, Dinesh/TextAndEmotions.csv'\n",
        "texts, labels = read_data_from_csv(file_path)\n",
        "\n",
        "# Create vocabulary and word_to_idx mapping\n",
        "vocab = set(' '.join(texts).split())\n",
        "word_to_idx = {word: idx + 1 for idx, word in enumerate(vocab)}  # Add 1 to reserve index 0 for padding\n",
        "\n",
        "# Update the dataset and pad sequences\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = EmotionDataset(train_texts, train_labels, word_to_idx)\n",
        "test_dataset = EmotionDataset(test_texts, test_labels, word_to_idx)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "\n",
        "model = EmotionAnalysisModel(vocab_size=len(word_to_idx) + 1, embedding_dim=50, hidden_dim=100, output_dim=len(set(labels))).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        text, label = batch['text'].to(device), batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        text, label = batch['text'].to(device), batch['label'].to(device)\n",
        "        output = model(text)\n",
        "        predictions = torch.argmax(output, dim=1)\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(label.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample inputs\n",
        "sample_inputs = [\n",
        "    \"I love the new feature!\",\n",
        "    \"I am satisfied with the customer service.\",\n",
        "    \"This movie is not good!\",\n",
        "    \"This software have lots of features.\",\n",
        "    \"Incredible customer experience!\",\n",
        "    \"I am anxious\"\n",
        "]\n",
        "\n",
        "# Predict emotions for each sample input\n",
        "for sample_input in sample_inputs:\n",
        "    # Preprocess the sample input\n",
        "    sample_input_indices = [word_to_idx[word] for word in sample_input.split() if word in word_to_idx]\n",
        "\n",
        "    # Check if the sample input is empty after filtering\n",
        "    if not sample_input_indices:\n",
        "        print(f\"No valid words found in the sample input: {sample_input}\")\n",
        "    else:\n",
        "        # Convert to tensor and add batch dimension\n",
        "        sample_input_tensor = torch.LongTensor(sample_input_indices).unsqueeze(0).to(device)\n",
        "\n",
        "        # Pass through the trained model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(sample_input_tensor)\n",
        "\n",
        "        # Interpret the model's output\n",
        "        predicted_class = torch.argmax(output, dim=1).item()\n",
        "\n",
        "        # Map the predicted class to the corresponding emotion label\n",
        "        emotion_labels = {0: \"Negative\", 1: \"Positive\"}\n",
        "        predicted_emotion = emotion_labels.get(predicted_class, \"Unknown\")\n",
        "\n",
        "        print(f\"The predicted emotion for the sample input '{sample_input}' is: {predicted_emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpUNOcYbClSn",
        "outputId": "1bc8114b-6847-4228-abfd-f267ee908476"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted emotion for the sample input 'I love the new feature!' is: Positive\n",
            "The predicted emotion for the sample input 'I am satisfied with the customer service.' is: Positive\n",
            "The predicted emotion for the sample input 'This movie is not good!' is: Negative\n",
            "The predicted emotion for the sample input 'This software have lots of features.' is: Positive\n",
            "The predicted emotion for the sample input 'Incredible customer experience!' is: Positive\n",
            "The predicted emotion for the sample input 'I am anxious' is: Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eJ993jop8d2-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}